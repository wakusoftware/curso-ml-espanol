{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wakusoftware/curso-ml-espanol/blob/master/C1%20-%20Aprendizaje%20Supervisado/C1_W1_Lab04_Descendiente_Gradiente.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "id": "c5e2ee6409a345bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup para Colab\n",
    "Si estás corriendo este Notebook en Google Colab corre la celda de abajo, de lo contrario ignórala."
   ],
   "id": "8c7601cac4ad390b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/wakusoftware/curso-ml-espanol.git\n",
    "\n",
    "%cd curso-ml-espanol/C1 - Aprendizaje Supervisado/\n",
    "\n",
    "!cp -r deeplearning.mplstyle /content/\n",
    "\n",
    "!cp -r images /content/\n",
    "\n",
    "!cp -r lab_utils_uni.py /content/\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "!rm -rf curso-ml-espanol/"
   ],
   "id": "528c99ff3afe97b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab: Descenso de Gradiente para Regresión Lineal\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images/C1_W1_L4_S1_Lecture_GD.jpeg\"  style=\"width:800px;height:200px;\" ></center>\n",
    "</figure>"
   ],
   "id": "3d015840591eea20"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Objetivos\n",
    "En este laboratorio:\n",
    "- automatizarás el proceso de optimización de $w$ y $b$ utilizando el descenso de gradiente."
   ],
   "id": "28900953e0e100a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Herramientas\n",
    "En este laboratorio, haremos uso de:\n",
    "- NumPy, una biblioteca popular para computación científica\n",
    "- Matplotlib, una biblioteca popular para graficar datos\n",
    "- rutinas de graficación en el archivo lab_utils.py en el directorio local."
   ],
   "id": "d114529ec173545d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ],
   "id": "103a4f3c8d85f539",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Planteamiento del problema\n",
    "\n",
    "Utilicemos los mismos dos puntos de datos que antes: una casa de 1000 pies cuadrados se vendió por 300,000 dólares y una casa de 2000 pies cuadrados se vendió por 500,000 dólares.\n",
    "\n",
    "| Tamaño (1000 pies cuadrados) | Precio (miles de dólares) |\n",
    "| ----------------------------- | -------------------------- |\n",
    "| 1                             | 300                        |\n",
    "| 2                             | 500                        |"
   ],
   "id": "fd227559155a720d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Cargar nuestro dataset\n",
    "x_train = np.array([1.0, 2.0])   # características\n",
    "y_train = np.array([300.0, 500.0])   # valor objetivo"
   ],
   "id": "98f23b45d5138ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.0.1\"></a>\n",
    "### compute_cost\n",
    "Esto se desarrolló en el último laboratorio. Lo necesitaremos de nuevo aquí."
   ],
   "id": "e55c32a2f471c922"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Función para calcular el costo o pérdida\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ],
   "id": "bd4ac05932931a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.1\"></a>\n",
    "## Resumen del descenso de gradiente\n",
    "Hasta ahora en este curso, has desarrollado un modelo lineal que predice $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "En la regresión lineal, utilizas datos de entrenamiento de entrada para ajustar los parámetros $w$, $b$ minimizando una medida del error entre nuestras predicciones $f_{w,b}(x^{(i)})$ y los datos reales $y^{(i)}$. La medida se llama el $costo$, $J(w,b)$. En el entrenamiento, mides el costo sobre todas nuestras muestras de entrenamiento $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$"
   ],
   "id": "d0c8b1c9a4e01cf2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "En la conferencia, se describió el *descenso de gradiente* como:\n",
    "\n",
    "$$\\begin{align*} \\text{repetir}&\\text{ hasta la convergencia:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "donde, los parámetros $w$, $b$ se actualizan simultáneamente.  \n",
    "El gradiente se define como:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aquí *simultáneamente* significa que calculas las derivadas parciales para todos los parámetros antes de actualizar cualquiera de los parámetros."
   ],
   "id": "3ebdbd08bf63d08f"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.2\"></a>\n",
    "## Implementar el Descenso de Gradiente\n",
    "Implementarás el algoritmo de descenso de gradiente para una característica. Necesitarás tres funciones.\n",
    "- `compute_gradient` implementando las ecuaciones (4) y (5) arriba mencionadas\n",
    "- `compute_cost` implementando la ecuación (2) arriba mencionada (código del laboratorio anterior)\n",
    "- `gradient_descent`, utilizando compute_gradient y compute_cost\n",
    "\n",
    "Convenciones:\n",
    "- El nombramiento de variables de Python que contienen derivadas parciales sigue este patrón, $\\frac{\\partial J(w,b)}{\\partial b}$ será `dj_db`.\n",
    "- w.r.t es With Respect To (Con Respecto A), como en derivada parcial de $J(wb)$ Con Respecto A $b$."
   ],
   "id": "97aac08e1b5ac269"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.3\"></a>\n",
    "### compute_gradient\n",
    "<a name='ex-01'></a>\n",
    "`compute_gradient` implementa (4) y (5) arriba mencionadas y devuelve $\\frac{\\partial J(w,b)}{\\partial w}$, $\\frac{\\partial J(w,b)}{\\partial b}$. Los comentarios incrustados describen las operaciones."
   ],
   "id": "8e537eb498de4534"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula el gradiente para la regresión lineal\n",
    "    Argumentos:\n",
    "      x (ndarray (m,)): Datos, m ejemplos\n",
    "      y (ndarray (m,)): valores objetivo\n",
    "      w,b (escalar)    : parámetros del modelo  \n",
    "    Devuelve\n",
    "      dj_dw (escalar): El gradiente del costo con respecto a los parámetros w\n",
    "      dj_db (escalar): El gradiente del costo con respecto al parámetro b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Número de ejemplos de entrenamiento\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ],
   "id": "f0df4893a20db121",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br/>"
   ],
   "id": "86176bc324697839"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Las clases describieron cómo el descenso de gradiente utiliza la derivada parcial del costo con respecto a un parámetro en un punto para actualizar ese parámetro.\n",
    "Vamos a usar nuestra función `compute_gradient` para encontrar y graficar algunas derivadas parciales de nuestra función de costo en relación con uno de los parámetros, $w_0$."
   ],
   "id": "f3e5fb1bb8a68fb8"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ],
   "id": "818682cf90b951fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Arriba, el gráfico de la izquierda muestra $\\frac{\\partial J(w,b)}{\\partial w}$ o la pendiente de la curva de costo relativa a $w$ en tres puntos. En el lado derecho del gráfico, la derivada es positiva, mientras que en el lado izquierdo es negativa. Debido a la forma de 'cuenco', las derivadas siempre guiarán el descenso de gradiente hacia el fondo donde el gradiente es cero.\n",
    "\n",
    "El gráfico de la izquierda tiene $b=100$ fijo. El descenso de gradiente utilizará tanto $\\frac{\\partial J(w,b)}{\\partial w}$ como $\\frac{\\partial J(w,b)}{\\partial b}$ para actualizar parámetros. El 'gráfico de flechas' a la derecha proporciona un medio para visualizar el gradiente de ambos parámetros. El tamaño de las flechas refleja la magnitud del gradiente en ese punto. La dirección y pendiente de la flecha refleja la proporción de $\\frac{\\partial J(w,b)}{\\partial w}$ y $\\frac{\\partial J(w,b)}{\\partial b}$ en ese punto.\n",
    "Nota que el gradiente apunta *lejos* del mínimo. Revisa la ecuación (3) arriba. El gradiente escalado se *resta* del valor actual de $w$ o $b$. Esto mueve el parámetro en una dirección que reducirá el costo."
   ],
   "id": "d09c0ffbaeac0a5e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.5\"></a>\n",
    "### Descenso de Gradiente\n",
    "Ahora que se pueden calcular los gradientes, el descenso de gradiente, descrito en la ecuación (3) arriba, se puede implementar a continuación en `gradient_descent`. Los detalles de la implementación se describen en los comentarios. A continuación, utilizarás esta función para encontrar valores óptimos de $w$ y $b$ en los datos de entrenamiento."
   ],
   "id": "dd550a779ff7ea2e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Realiza el descenso de gradiente para ajustar w,b. Actualiza w,b tomando\n",
    "    num_iters pasos de gradiente con una tasa de aprendizaje alpha\n",
    "    \n",
    "    Argumentos:\n",
    "      x (ndarray (m,))  : Datos, m ejemplos \n",
    "      y (ndarray (m,))  : valores objetivo\n",
    "      w_in, b_in (escalar): valores iniciales de los parámetros del modelo  \n",
    "      alpha (float):     Tasa de aprendizaje\n",
    "      num_iters (int):   número de iteraciones para ejecutar el descenso de gradiente\n",
    "      cost_function:     función a llamar para producir el costo\n",
    "      gradient_function: función a llamar para producir el gradiente\n",
    "      \n",
    "    Devuelve:\n",
    "      w (escalar): Valor actualizado del parámetro después de ejecutar el descenso de gradiente\n",
    "      b (escalar): Valor actualizado del parámetro después de ejecutar el descenso de gradiente\n",
    "      J_history (Lista): Historial de valores de costo\n",
    "      p_history (lista): Historial de parámetros [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w_in) # evitar modificar w_in global\n",
    "    # Un arreglo para almacenar el costo J y los w's en cada iteración principalmente para graficar luego\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calcular el gradiente y actualizar los parámetros usando gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Actualizar Parámetros usando la ecuación (3) arriba\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Guardar el costo J en cada iteración\n",
    "        if i<100000:      # prevenir agotamiento de recursos \n",
    "            J_history.append(cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Imprimir el costo cada ciertos intervalos 10 veces o tantas iteraciones si son < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteración {i:4}: Costo {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #devolver w y el historial de J,w para graficar\n"
   ],
   "id": "845ab57501353cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# inicializar parámetros\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# algunas configuraciones de descenso de gradiente\n",
    "iteraciones = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# ejecutar el descenso de gradiente\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iteraciones, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) encontrados por descenso de gradiente: ({w_final:8.4f},{b_final:8.4f})\")\n"
   ],
   "id": "fa073e3ee9501725",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tómate un momento y nota algunas características del proceso de descenso de gradiente impreso arriba.\n",
    "\n",
    "- El costo comienza grande y disminuye rápidamente, como se describió en la diapositiva de la clase.\n",
    "- Las derivadas parciales, `dj_dw`, y `dj_db` también se hacen más pequeñas, rápidamente al principio y luego más lentamente. A medida que el proceso se acerca al 'fondo del tazón', el progreso es más lento debido al menor valor de la derivada en ese punto.\n",
    "- El progreso se ralentiza aunque la tasa de aprendizaje, alfa, permanece fija."
   ],
   "id": "346b79b6c00c527a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Costo versus iteraciones del descenso de gradiente\n",
    "Un gráfico del costo versus las iteraciones es una medida útil del progreso en el descenso de gradiente. El costo siempre debería disminuir en ejecuciones exitosas. El cambio en el costo es tan rápido inicialmente, que es útil graficar el descenso inicial en una escala diferente que el descenso final. En los gráficos a continuación, nota la escala del costo en los ejes y el paso de iteración."
   ],
   "id": "a3562e37d44d6c1b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# graficar costo versus iteración\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Costo vs. iteración (inicio)\");  ax2.set_title(\"Costo vs. iteración (final)\")\n",
    "ax1.set_ylabel('Costo')            ;  ax2.set_ylabel('Costo') \n",
    "ax1.set_xlabel('paso de iteración')  ;  ax2.set_xlabel('paso de iteración') \n",
    "plt.show()\n"
   ],
   "id": "e454fd36ccd48706",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Predicciones\n",
    "Ahora que has descubierto los valores óptimos para los parámetros $w$ y $b$, ahora puedes usar el modelo para predecir valores de viviendas basados en nuestros parámetros aprendidos. Como se esperaba, los valores predichos son casi los mismos que los valores de entrenamiento para la misma vivienda. Además, el valor que no está en la predicción está en línea con el valor esperado."
   ],
   "id": "1411e45c92e813f2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(f\"Predicción de casa de 1000 pies cuadrados {w_final*1.0 + b_final:0.1f} Mil dólares\")\n",
    "print(f\"Predicción de casa de 1200 pies cuadrados {w_final*1.2 + b_final:0.1f} Mil dólares\")\n",
    "print(f\"Predicción de casa de 2000 pies cuadrados {w_final*2.0 + b_final:0.1f} Mil dólares\")"
   ],
   "id": "427152f691710495",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.6\"></a>\n",
    "## Graficación\n",
    "Puedes mostrar el progreso del descenso de gradiente durante su ejecución al graficar el costo sobre las iteraciones en un gráfico de contorno del costo(w,b)."
   ],
   "id": "ca10b27fd2df98b8"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax)"
   ],
   "id": "a08a5d7347a13657",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Arriba, el gráfico de contorno muestra el $costo(w,b)$ sobre un rango de $w$ y $b$. Los niveles de costo están representados por los anillos. Superpuesto, usando flechas rojas, está el camino del descenso de gradiente. Aquí hay algunas cosas a tener en cuenta:\n",
    "- El camino hace un progreso constante (monotónico) hacia su objetivo.\n",
    "- Los pasos iniciales son mucho más grandes que los pasos cerca del objetivo."
   ],
   "id": "655d3e8e6bf40eff"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "**Acercándonos**, podemos ver los pasos finales del descenso de gradiente. Nota que la distancia entre pasos se reduce a medida que el gradiente se acerca a cero.",
   "id": "5729fff0f3ad5f08"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n",
    "            contours=[1,5,10,20],resolution=0.5)"
   ],
   "id": "9959ac96d60e57e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_40291_2.7.1\"></a>\n",
    "### Aumento de la Tasa de Aprendizaje\n",
    "\n",
    "<figure>\n",
    " <img align=\"left\", src=\"./images/C1_W1_Lab03_alpha_too_big.jpeg\"   style=\"width:340px;height:240px;\" >\n",
    "</figure>\n",
    "En la conferencia, hubo una discusión relacionada con el valor adecuado de la tasa de aprendizaje, $\\alpha$ en la ecuación(3). Cuanto mayor es $\\alpha$, más rápido el descenso de gradiente convergerá a una solución. Pero, si es demasiado grande, el descenso de gradiente divergerá. Arriba tienes un ejemplo de una solución que converge bien.\n",
    "\n",
    "Intentemos aumentar el valor de $\\alpha$ y veamos qué sucede:"
   ],
   "id": "beccc6287c4eb7a6"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# inicializar parámetros\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# establecer alpha a un valor grande\n",
    "iteraciones = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# ejecutar el descenso de gradiente\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iteraciones, compute_cost, compute_gradient)\n"
   ],
   "id": "c9b3591b4791e459",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Arriba, $w$ y $b$ están rebotando de un lado a otro entre positivo y negativo con el valor absoluto aumentando con cada iteración. Además, en cada iteración $\\frac{\\partial J(w,b)}{\\partial w}$ cambia de signo y el costo está aumentando en lugar de disminuir. Esto es una clara señal de que *la tasa de aprendizaje es demasiado grande* y la solución está divergiendo.\n",
    "Visualicemos esto con un gráfico."
   ],
   "id": "92078cf791ec0309"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plt_divergence(p_hist, J_hist,x_train, y_train)\n",
    "plt.show()"
   ],
   "id": "a2ee857622eafeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Arriba, el gráfico de la izquierda muestra la progresión de $w$ durante los primeros pasos del descenso de gradiente. $w$ oscila de positivo a negativo y el costo crece rápidamente. El Descenso de Gradiente opera tanto en $w$ como en $b$ simultáneamente, por lo que se necesita el gráfico 3D de la derecha para la imagen completa.",
   "id": "e1041249ed9ad490"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ¡Felicidades!\n",
    "En este laboratorio:\n",
    "- profundizaste en los detalles del descenso de gradiente para una variable única.\n",
    "- desarrollaste una rutina para calcular el gradiente\n",
    "- visualizaste qué es el gradiente\n",
    "- completaste una rutina de descenso de gradiente\n",
    "- utilizaste el descenso de gradiente para encontrar parámetros\n",
    "- examinaste el impacto de ajustar la tasa de aprendizaje"
   ],
   "id": "6d1dc2249f7f6725"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "id": "11ab2d8069aa4b1f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
