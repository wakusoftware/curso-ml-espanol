{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wakusoftware/curso-ml-espanol/blob/master/C1%20-%20Aprendizaje%20Supervisado/W2/C1_W2_Lab03_Escalado_Caracteristicas_Tasa_Aprendizaje.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "id": "bcd33177245ec4dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup para Colab\n",
    "Si estás corriendo este Notebook en Google Colab corre la celda de abajo, de lo contrario ignórala."
   ],
   "id": "a0e2fae8d23b806b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/wakusoftware/curso-ml-espanol.git\n",
    "\n",
    "%cd curso-ml-espanol/C1 - Aprendizaje Supervisado/W2/\n",
    "\n",
    "!cp lab_utils_common.py /content/\n",
    "\n",
    "!cp lab_utils_multi.py /content/\n",
    "\n",
    "!cp deeplearning.mplstyle /content/\n",
    "\n",
    "!cp -r data /content/\n",
    "\n",
    "!cp -r images /content/\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "!rm -rf curso-ml-espanol/"
   ],
   "id": "788cd139d1541fc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "# Lab: Escalado de características y tasa de aprendizaje (Multivariable)",
   "id": "f3ce56a6533b612c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Objetivos\n",
    "En este laboratorio vas a:\n",
    "- Utilizar las rutinas de múltiples variables desarrolladas en el laboratorio anterior\n",
    "- ejecutar el Descenso del Gradiente en un conjunto de datos con múltiples características\n",
    "- explorar el impacto de la *tasa de aprendizaje alfa* en el descenso del gradiente\n",
    "- mejorar el rendimiento del descenso del gradiente mediante el *escalado de características* usando la normalización z-score"
   ],
   "id": "2ebc657c82eba6fd"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Herramientas\n",
    "Utilizarás las funciones desarrolladas en el último laboratorio, así como matplotlib y NumPy."
   ],
   "id": "b907aed28681c69c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import  load_house_data, run_gradient_descent \n",
    "from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\n",
    "from lab_utils_common import dlc\n",
    "np.set_printoptions(precision=2)\n",
    "plt.style.use('./deeplearning.mplstyle')\n"
   ],
   "id": "78bf0b781b15a3b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notación\n",
    "\n",
    "|Notación General <br />  | Descripción| Python (si aplica) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | escalar, no en negrita                                                      ||\n",
    "| $\\mathbf{a}$ | vector, en negrita                                                 ||\n",
    "| $\\mathbf{A}$ | matriz, en negrita y mayúscula                                         ||\n",
    "| **Regresión** |         |    |     |\n",
    "|  $\\mathbf{X}$ | matriz de ejemplos de entrenamiento                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | objetivos de los ejemplos de entrenamiento                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | Ejemplo de entrenamiento $i_{th}$ | `X[i]`, `y[i]`|\n",
    "| m | número de ejemplos de entrenamiento | `m`|\n",
    "| n | número de características en cada ejemplo | `n`|\n",
    "|  $\\mathbf{w}$  |  parámetro: peso                       | `w`    |\n",
    "|  $b$           |  parámetro: sesgo                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | El resultado de la evaluación del modelo en $\\mathbf{x}^{(i)}$ parametrizado por $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$| el gradiente o derivada parcial del costo con respecto a un parámetro $w_j$ |`dj_dw[j]`| \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$| el gradiente o derivada parcial del costo con respecto a un parámetro $b$| `dj_db`|"
   ],
   "id": "f4faf5bc35c5076c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Planteamiento del Problema\n",
    "\n",
    "Como en los laboratorios anteriores, utilizarás el ejemplo motivador de la predicción de precios de viviendas. El conjunto de datos de entrenamiento contiene muchos ejemplos con 4 características (tamaño, habitaciones, pisos y edad) mostrados en la tabla a continuación. Nota, en este laboratorio, la característica de Tamaño está en pies cuadrados mientras que en laboratorios anteriores se utilizaban 1000 pies cuadrados. Este conjunto de datos es más grande que el del laboratorio anterior.\n",
    "\n",
    "Nos gustaría construir un modelo de regresión lineal utilizando estos valores para que luego podamos predecir el precio de otras casas - digamos, una casa de 1200 pies cuadrados, 3 habitaciones, 1 piso, 40 años de antigüedad.\n",
    "\n",
    "## Conjunto de Datos:\n",
    "| Tamaño (pies cuadrados) | Número de Habitaciones  | Número de Pisos | Edad de la Vivienda | Precio (miles de dólares)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|----------------------- |  \n",
    "| 952             | 2                   | 1                | 65           | 271.5                  |  \n",
    "| 1244            | 3                   | 2                | 64           | 232                    |  \n",
    "| 1947            | 3                   | 2                | 17           | 509.8                  |  \n",
    "| ...             | ...                 | ...              | ...          | ...                    |"
   ],
   "id": "4c46a284c71e74ec"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# cargar el conjunto de datos\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['tamaño(pies cuadrados)','habitaciones','pisos','edad']"
   ],
   "id": "12610c59c56a78cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Vamos a visualizar el dataset y sus características graficando cada característica frente al precio.",
   "id": "681527097f52cf53"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i], y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Precio (miles)\")\n",
    "plt.show()\n"
   ],
   "id": "108f00e1f354c05a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Graficar cada característica frente al objetivo, el precio, proporciona alguna indicación de qué características tienen la mayor influencia en el precio. Arriba, el aumento del tamaño también aumenta el precio. Las habitaciones y los pisos no parecen tener un impacto fuerte en el precio. Las casas más nuevas tienen precios más altos que las casas más antiguas.",
   "id": "ce5f5a40cd828978"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "## Descenso del Gradiente con Múltiples Variables\n",
    "Aquí están las ecuaciones que desarrollaste en el último laboratorio sobre el descenso del gradiente para múltiples variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repetir}&\\text{ hasta la convergencia:} \\; \\lbrace \\newline\\;\n",
    "& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{para j = 0..n-1}\\newline\n",
    "&b\\ \\ := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "donde, n es el número de características, los parámetros $w_j$,  $b$, se actualizan simultáneamente y donde  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "* m es el número de ejemplos de entrenamiento en el conjunto de datos\n",
    "\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ es la predicción del modelo, mientras que $y^{(i)}$ es el valor objetivo"
   ],
   "id": "19ee7c849c6f4f59"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tasa de Aprendizaje\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_learningrate.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "Las clases discutieron algunos de los problemas relacionados con la configuración de la tasa de aprendizaje $\\alpha$. La tasa de aprendizaje controla el tamaño de la actualización de los parámetros. Vea la ecuación (1) arriba. Es compartida por todos los parámetros.\n",
    "\n",
    "Vamos a ejecutar el descenso del gradiente y probar algunas configuraciones de $\\alpha$ en nuestro conjunto de datos."
   ],
   "id": "bd08fa959dd84b10"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $\\alpha$ = 9.9e-7"
   ],
   "id": "3f3994d2b110732a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# establecer alpha a 9.9e-7\n",
    "_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)"
   ],
   "id": "453fd95381bfd291",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Parece que la tasa de aprendizaje es demasiado alta. La solución no converge. El costo está *aumentando* en lugar de disminuir. Vamos a graficar el resultado:",
   "id": "dc5638fc8e10eb79"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plot_cost_i_w(X_train, y_train, hist)"
   ],
   "id": "3f42032b3e9e2d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "El gráfico de la derecha muestra el valor de uno de los parámetros, $w_0$. En cada iteración, está sobrepasando el valor óptimo y, como resultado, el costo termina *aumentando* en lugar de acercarse al mínimo. Note que esta no es una imagen completamente precisa, ya que hay 4 parámetros que se modifican en cada paso y no solo uno. Este gráfico solo muestra $w_0$ con los otros parámetros fijados en valores benignos. En este y en gráficos posteriores, podrías notar que las líneas azul y naranja están ligeramente desplazadas.",
   "id": "dd2dc2f625e5ca62"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $\\alpha$ = 9e-7\n",
    "Probemos con un valor un poco menor y veamos qué sucede."
   ],
   "id": "d9cd3bed855a638b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# establecer alpha a 9e-7\n",
    "_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)\n"
   ],
   "id": "a5f8a5582f2aeac4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "El costo está disminuyendo a lo largo de la ejecución, lo que muestra que el valor de alpha no es demasiado grande.",
   "id": "f17b1f93f55393b3"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plot_cost_i_w(X_train, y_train, hist)"
   ],
   "id": "a3fe423a1c61377",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A la izquierda, puedes ver que el costo está disminuyendo como debería. A la derecha, puedes ver que $w_0$ todavía oscila alrededor del mínimo, pero está disminuyendo en cada iteración en lugar de aumentar. Observa que `dj_dw[0]` cambia de signo en cada iteración ya que `w[0]` salta por encima del valor óptimo.\n",
    "Este valor de alpha convergerá. Puedes variar el número de iteraciones para ver cómo se comporta."
   ],
   "id": "13cbfd152668d51a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### $\\alpha$ = 1e-7\n",
    "Probemos con un valor aún más pequeño para $\\alpha$ y veamos qué sucede."
   ],
   "id": "4e5cbe7e76bf3840"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# establecer alpha a 1e-7\n",
    "_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)"
   ],
   "id": "e5056b58a751b2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "El costo está disminuyendo a lo largo de la ejecución, lo que muestra que $\\alpha$ no es demasiado grande.",
   "id": "97eba64cff3645ea"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plot_cost_i_w(X_train,y_train,hist)"
   ],
   "id": "c81671a4d14f3794",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "A la izquierda, puedes ver que el costo está disminuyendo como debería. A la derecha puedes ver que $w_0$ está disminuyendo sin cruzar el mínimo. Observa que `dj_w0` es negativo durante toda la ejecución. Esta solución también convergerá, aunque no tan rápidamente como en el ejemplo anterior.",
   "id": "2427797d954d3794"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Escalado de Características\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_featurescalingheader.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "Las conferencias describieron la importancia de reescalar el conjunto de datos para que las características tengan un rango similar.\n",
    "Si estás interesado en los detalles de por qué esto es así, haz clic en el encabezado 'detalles' a continuación. Si no, la sección siguiente te guiará a través de una implementación de cómo realizar el escalado de características."
   ],
   "id": "a92ef64649da28bc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3' color='darkgreen'><b>Detalles</b></font>\n",
    "</summary>\n",
    "\n",
    "Volvamos a mirar la situación con $\\alpha$ = 9e-7. Este valor está bastante cerca del máximo que podemos establecer para $\\alpha$ sin que diverja. Esta es una ejecución corta que muestra las primeras iteraciones:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_ShortRun.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "\n",
    "Arriba, aunque el costo está disminuyendo, es claro que $w_0$ está progresando más rápidamente que los otros parámetros debido a su gradiente mucho mayor.\n",
    "\n",
    "El gráfico a continuación muestra el resultado de una ejecución muy larga con $\\alpha$ = 9e-7. Esto tarda varias horas.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_LongRun.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "    \n",
    "Arriba, puedes ver que el costo disminuyó lentamente después de su reducción inicial. Observa la diferencia entre `w0` y `w1`, `w2`, `w3`, así como `dj_dw0` y `dj_dw1-3`. `w0` alcanza su valor casi final muy rápidamente y `dj_dw0` ha disminuido rápidamente a un valor pequeño, mostrando que `w0` está cerca del valor final. Los otros parámetros se redujeron mucho más lentamente.\n",
    "\n",
    "¿Por qué es esto? ¿Hay algo que podamos mejorar? Vea abajo:\n",
    "<figure>\n",
    "    <center> <img src=\"./images/C1_W2_Lab06_scale.PNG\"   ></center>\n",
    "</figure>   \n",
    "\n",
    "La figura anterior muestra por qué las actualizaciones de $w$ son desiguales.\n",
    "- $\\alpha$ es compartido por todas las actualizaciones de parámetros ($w$ y $b$).\n",
    "- el término de error común se multiplica por las características para los $w$ (no para $b$).\n",
    "- las características varían significativamente en magnitud, lo que hace que algunas características se actualicen mucho más rápido que otras. En este caso, $w_0$ se multiplica por 'tamaño (pies cuadrados)', que generalmente es > 1000, mientras que $w_1$ se multiplica por 'número de habitaciones', que generalmente es 2-4.\n",
    "    \n",
    "La solución es el Escalado de Características."
   ],
   "id": "e50bc6fd82be0be1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "En las clases se discutieron tres técnicas diferentes:\n",
    "- Escalado de características, esencialmente dividiendo cada característica positiva por su valor máximo, o más generalmente, reescalar cada característica utilizando tanto su valor mínimo como máximo con la fórmula \\((x - \\text{min}) / (\\text{max} - \\text{min})\\). Ambas formas normalizan las características al rango de -1 y 1, donde el primer método funciona para características positivas, es simple y sirve bien para el ejemplo de la conferencia, y el segundo método funciona para cualquier característica.\n",
    "- Normalización por media: \\(x_i := \\dfrac{x_i - \\mu_i}{\\text{max} - \\text{min}} \\)\n",
    "- Normalización Z-score, la cual exploraremos a continuación."
   ],
   "id": "7029d3deb0367f07"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Normalización Z-score\n",
    "Después de la normalización Z-score, todas las características tendrán una media de 0 y una desviación estándar de 1.\n",
    "\n",
    "Para implementar la normalización Z-score, ajusta tus valores de entrada como se muestra en esta fórmula:\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ \n",
    "donde $j$ selecciona una característica o una columna en la matriz $\\mathbf{X}$. $\\mu_j$ es la media de todos los valores para la característica (j) y $\\sigma_j$ es la desviación estándar de la característica (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Nota de implementación:** Al normalizar las características, es importante\n",
    "almacenar los valores utilizados para la normalización - el valor medio y la desviación estándar utilizados para los cálculos. Después de aprender los parámetros\n",
    "del modelo, a menudo queremos predecir los precios de casas que no hemos\n",
    "visto antes. Dado un nuevo valor de x (área de la sala y número de dormitorios), primero debemos normalizar x usando la media y la desviación estándar\n",
    "que habíamos calculado previamente del conjunto de entrenamiento.\n",
    "\n",
    "**Implementación**"
   ],
   "id": "99047d0af3836469"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    calcula X, normalizado por z-score por columna\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : datos de entrada, m ejemplos, n características\n",
    "      \n",
    "    Retorna:\n",
    "      X_norm (ndarray (m,n)): datos de entrada normalizados por columna\n",
    "      mu (ndarray (n,))     : media de cada característica\n",
    "      sigma (ndarray (n,))  : desviación estándar de cada característica\n",
    "    \"\"\"\n",
    "    # encontrar la media de cada columna/característica\n",
    "    mu     = np.mean(X, axis=0)                 # mu tendrá forma (n,)\n",
    "    # encontrar la desviación estándar de cada columna/característica\n",
    "    sigma  = np.std(X, axis=0)                  # sigma tendrá forma (n,)\n",
    "    # elemento a elemento, resta mu para esa columna de cada ejemplo, divide por std para esa columna\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    " \n",
    "# verificar nuestro trabajo\n",
    "# from sklearn.preprocessing import scale\n",
    "# scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n"
   ],
   "id": "2ee1ce952a6eb558",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Veamos los pasos involucrados en la normalización Z-score. El gráfico a continuación muestra el proceso de transformación paso a paso.",
   "id": "9f59da86936fc522"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "mu = np.mean(X_train,axis=0)\n",
    "sigma = np.std(X_train,axis=0)\n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"sin normalizar\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Normalizado Z-score\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribución de características antes, durante y después de la normalización\")\n",
    "plt.show()"
   ],
   "id": "e7c69c096225aefe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El gráfico anterior muestra la relación entre dos de los parámetros del conjunto de entrenamiento, \"edad\" y \"tamaño(sqft)\". *Estos se representan con escala igual*.\n",
    "- Izquierda: Sin normalizar: El rango de valores o la varianza de la característica 'tamaño(sqft)' es mucho más grande que la de 'edad'.\n",
    "- Medio: El primer paso elimina la media o el valor promedio de cada característica. Esto deja características centradas alrededor de cero. Es difícil ver la diferencia para la característica 'edad', pero 'tamaño(sqft)' está claramente alrededor de cero.\n",
    "- Derecha: El segundo paso divide por la varianza. Esto deja ambas características centradas en cero con una escala similar."
   ],
   "id": "384aa46d177d83d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Normalicemos los datos y comparemos con los datos originales.",
   "id": "eab4a1bfc8347b23"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Normalizar las características originales\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Rango pico a pico por columna en X sin procesar: {np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Rango pico a pico por columna en X normalizado: {np.ptp(X_norm,axis=0)}\")"
   ],
   "id": "deb3eee23ab4682a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "El rango pico a pico de cada columna se reduce de un factor de miles a un factor de 2-3 mediante la normalización.",
   "id": "5293a0c2fb272756"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i], X_train[:, i])\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"conteo\")\n",
    "fig.suptitle(\"distribución de características antes de la normalización\")\n",
    "plt.show()\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i], X_norm[:, i])\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"conteo\")\n",
    "fig.suptitle(\"distribución de características después de la normalización\")\n",
    "plt.show()"
   ],
   "id": "430a8dd095f1ff52",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Observa que arriba, el rango de los datos normalizados (eje x) está centrado alrededor de cero y aproximadamente entre +/- 2. Lo más importante es que el rango es similar para cada característica.",
   "id": "16e78bb8027e631d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Volvamos a ejecutar nuestro algoritmo de descenso de gradiente con datos normalizados.\n",
    "Nota el **valor mucho más grande de alpha**. Esto acelerará el descenso de gradiente."
   ],
   "id": "da5afb807178583f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )"
   ],
   "id": "99b790e3e5b66a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Las características escaladas obtienen resultados **mucho, mucho más rápido** y con gran precisión. Observa que el gradiente de cada parámetro es muy pequeño al final de esta ejecución relativamente corta. Una tasa de aprendizaje de 0.1 es un buen punto de partida para la regresión con características normalizadas.\n",
    "Ahora grafiquemos nuestras predicciones frente a los valores objetivo. Ten en cuenta que la predicción se realiza utilizando la característica normalizada, mientras que el gráfico se muestra utilizando los valores de características originales."
   ],
   "id": "7f9beed5063341c0"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Predice el objetivo utilizando características normalizadas\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
    "\n",
    "# Grafica las predicciones y los objetivos frente a las características originales    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'objetivo')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'predicción')\n",
    "ax[0].set_ylabel(\"Precio\"); ax[0].legend();\n",
    "fig.suptitle(\"objetivo versus predicción usando un modelo normalizado por puntuación z\")\n",
    "plt.show()"
   ],
   "id": "ac85b471e927f677",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Los resultados lucen bien. Algunos puntos a tener en cuenta:\n",
    "- Con múltiples características, ya no podemos tener un solo gráfico que muestre los resultados versus características.\n",
    "- Al generar el gráfico, se utilizaron las características normalizadas. Cualquier predicción utilizando los parámetros aprendidos de un conjunto de entrenamiento normalizado también debe estar normalizada."
   ],
   "id": "2db89ae46be6de0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Predicción**\n",
    "El propósito de generar nuestro modelo es usarlo para predecir los precios de viviendas que no están en el conjunto de datos. Vamos a predecir el precio de una casa con 1200 pies cuadrados, 3 dormitorios, 1 piso, y 40 años de antigüedad. Recuerda que debes normalizar los datos con la media y la desviación estándar obtenidas cuando se normalizó el conjunto de datos de entrenamiento."
   ],
   "id": "fb7d0bb83658a358"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Primero, normalizamos nuestro ejemplo.\n",
    "x_casa = np.array([1200, 3, 1, 40])\n",
    "x_casa_norm = (x_casa - X_mu) / X_sigma\n",
    "print(x_casa_norm)\n",
    "x_prediccion_casa = np.dot(x_casa_norm, w_norm) + b_norm\n",
    "print(f\" precio predicho de una casa con 1200 pies cuadrados, 3 dormitorios, 1 piso, 40 años de antigüedad = ${x_prediccion_casa*1000:0.0f}\")"
   ],
   "id": "90b86e6968a45007",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Contornos de Costo**  \n",
    "<img align=\"left\" src=\"./images/C1_W2_Lab06_contours.PNG\"   style=\"width:240px;\" >Otra forma de ver el escalado de características es en términos de los contornos de costo. Cuando las escalas de características no coinciden, la gráfica del costo versus parámetros en un gráfico de contorno es asimétrica.\n",
    "\n",
    "En la gráfica a continuación, se ajusta la escala de los parámetros. La gráfica de la izquierda es el gráfico de contorno de costo de w[0], los pies cuadrados versus w[1], el número de dormitorios antes de normalizar las características. La gráfica es tan asimétrica que las curvas que completan los contornos no son visibles. En contraste, cuando las características están normalizadas, el contorno de costo es mucho más simétrico. El resultado es que las actualizaciones a los parámetros durante el descenso del gradiente pueden avanzar de manera igual para cada parámetro."
   ],
   "id": "f54785ab204cf5ed"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plt_equal_scale(X_train, X_norm, y_train)"
   ],
   "id": "84b58253c8ae7650",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ¡Felicidades!\n",
    "En este laboratorio:\n",
    "- utilizaste los procedimientos para regresión lineal con múltiples características que desarrollaste en laboratorios anteriores.\n",
    "- exploraste el impacto de la tasa de aprendizaje $\\alpha$ en la convergencia.\n",
    "- descubriste el valor del escalado de características mediante la normalización de puntuaciones z para acelerar la convergencia."
   ],
   "id": "3c177fc815bf611e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reconocimientos\n",
    "Los datos de viviendas se derivaron del conjunto de datos de viviendas de [Ames](http://jse.amstat.org/v19n3/decock.pdf) compilado por Dean De Cock para su uso en educación en ciencia de datos."
   ],
   "id": "b46f5a60334c8d8d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "id": "3ecedb3e8531aff8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
