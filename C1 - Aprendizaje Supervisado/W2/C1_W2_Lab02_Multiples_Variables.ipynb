{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wakusoftware/curso-ml-espanol/blob/master/C1%20-%20Aprendizaje%20Supervisado/W2/C1_W2_Lab01_Numpy_Vectorizacion.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "id": "8e231ad86881efe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup para Colab\n",
    "Si estás corriendo este Notebook en Google Colab corre la celda de abajo, de lo contrario ignórala."
   ],
   "id": "e2ab744e98f3d7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/wakusoftware/curso-ml-espanol.git\n",
    "\n",
    "%cd curso-ml-espanol/C1 - Aprendizaje Supervisado/W2/\n",
    "\n",
    "!cp lab_utils_common.py /content/\n",
    "\n",
    "!cp lab_utils_multi.py /content/\n",
    "\n",
    "!cp deeplearning.mplstyle /content/\n",
    "\n",
    "!cp -r data /content/\n",
    "\n",
    "!cp -r images /content/\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "!rm -rf curso-ml-espanol/"
   ],
   "id": "5b198c37f95b843d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laboratorio: Regresión Lineal Múltiple\n",
    "\n",
    "En este laboratorio, extenderás las estructuras de datos y las rutinas desarrolladas anteriormente para soportar múltiples características. Varias rutinas se actualizan haciendo que el laboratorio parezca extenso, pero solo realiza ajustes menores a las rutinas anteriores, lo que facilita su revisión.\n",
    "# Contenido\n",
    "- [&nbsp;&nbsp;1.1 Objetivos](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 Herramientas](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 Notación](#toc_15456_1.3)\n",
    "- [2 Planteamiento del Problema](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 Matriz X que contiene nuestros ejemplos](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 Vector de parámetros w, b](#toc_15456_2.2)\n",
    "- [3 Predicción del Modelo con Múltiples Variables](#toc_15456_3)\n",
    "- [&nbsp;&nbsp;3.1 Predicción individual elemento por elemento](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 Predicción individual, vector](#toc_15456_3.2)\n",
    "- [4 Cálculo del Costo con Múltiples Variables](#toc_15456_4)\n",
    "- [5 Descenso del Gradiente con Múltiples Variables](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 Calcular el Gradiente con Múltiples Variables](#toc_15456_5.1)\n",
    "- [&nbsp;&nbsp;5.2 Descenso del Gradiente con Múltiples Variables](#toc_15456_5.2)\n",
    "- [6 Felicitaciones](#toc_15456_6)"
   ],
   "id": "413ee08625d33256"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_1.1\"></a>\n",
    "## 1.1 Objetivos\n",
    "- Extender nuestras rutinas del modelo de regresión para soportar múltiples características\n",
    "    - Extender las estructuras de datos para soportar múltiples características\n",
    "    - Reescribir las rutinas de predicción, costo y gradiente para soportar múltiples características\n",
    "    - Utilizar NumPy `np.dot` para vectorizar sus implementaciones por velocidad y simplicidad"
   ],
   "id": "fb148706d5c84eef"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 Herramientas\n",
    "En este laboratorio, haremos uso de: \n",
    "- NumPy, una biblioteca popular para computación científica\n",
    "- Matplotlib, una biblioteca popular para la visualización de datos"
   ],
   "id": "f1c8084a6baec539"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # precisión reducida en la visualización de arrays de numpy"
   ],
   "id": "997bdded452f591f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notación\n",
    "Aquí tienes un resumen de algunas de las notaciones que encontrarás, actualizadas para múltiples características.\n",
    "\n",
    "|Notación General <img width=70/> | Descripción<img width=350/>| Python (si aplica) |\n",
    "|: ------------|: ------------------------------------------------------------|:----------------|\n",
    "| $a$ | escalar, no en negrita                                                      ||\n",
    "| $\\mathbf{a}$ | vector, en negrita                                                 ||\n",
    "| $\\mathbf{A}$ | matriz, en negrita y mayúscula                                         ||\n",
    "| **Regresión** |         |    |     |\n",
    "|  $\\mathbf{X}$ | matriz de ejemplos de entrenamiento                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | objetivos de los ejemplos de entrenamiento                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | Ejemplo de entrenamiento $i_{th}$ | `X[i]`, `y[i]`|\n",
    "| m | número de ejemplos de entrenamiento | `m`|\n",
    "| n | número de características en cada ejemplo | `n`|\n",
    "|  $\\mathbf{w}$  |  parámetro: peso                       | `w`    |\n",
    "|  $b$           |  parámetro: sesgo                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | El resultado de la evaluación del modelo en $\\mathbf{x}^{(i)}$ parametrizado por $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` |"
   ],
   "id": "962396f5e096c55a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "# 2 Planteamiento del Problema\n",
    "\n",
    "Utilizarás el ejemplo motivador de predicción de precios de viviendas. El conjunto de datos de entrenamiento contiene tres ejemplos con cuatro características (tamaño, habitaciones, pisos y edad) mostrados en la tabla a continuación. Ten en cuenta que, a diferencia de los laboratorios anteriores, el tamaño está en pies cuadrados en lugar de 1000 pies cuadrados. ¡Esto causa un problema, que resolverás en el próximo laboratorio!\n",
    "\n",
    "| Tamaño (pies cuadrados) | Número de habitaciones  | Número de pisos | Edad de la vivienda | Precio (miles de dólares)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "Construirás un modelo de regresión lineal utilizando estos valores para que luego puedas predecir el precio de otras casas. Por ejemplo, una casa de 1200 pies cuadrados, 3 habitaciones, 1 piso, 40 años de antigüedad.\n",
    "\n",
    "Por favor, ejecuta la siguiente celda de código para crear tus variables `X_train` y `y_train`."
   ],
   "id": "6e49b8c45aedb2c9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ],
   "id": "487eee737f561c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_2.1\"></a>\n",
    "## 2.1 Matriz X que contiene nuestros ejemplos\n",
    "Similar a la tabla anterior, los ejemplos se almacenan en una matriz de NumPy `X_train`. Cada fila de la matriz representa un ejemplo. Cuando tienes $m$ ejemplos de entrenamiento (en nuestro ejemplo $m$ es tres), y hay $n$ características (cuatro en nuestro ejemplo), $\\mathbf{X}$ es una matriz con dimensiones ($m$, $n$) (m filas, n columnas).\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notación:\n",
    "- $\\mathbf{x}^{(i)}$ es el vector que contiene el ejemplo i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots, x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ es el elemento j en el ejemplo i. El superíndice entre paréntesis indica el número de ejemplo mientras que el subíndice representa un elemento.\n",
    "\n",
    "Muestra los datos de entrada."
   ],
   "id": "82a1ebd1e2fda5c0"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# los datos están almacenados en un array/matriz de numpy\n",
    "print(f\"Forma de X: {X_train.shape}, Tipo de X:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"Forma de y: {y_train.shape}, Tipo de y:{type(y_train)})\")\n",
    "print(y_train)"
   ],
   "id": "6c8a0f86471e97b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.2 Vector de parámetros w, b\n",
    "\n",
    "* $\\mathbf{w}$ es un vector con $n$ elementos.\n",
    "  - Cada elemento contiene el parámetro asociado con una característica.\n",
    "  - en nuestro conjunto de datos, n es 4.\n",
    "  - conceptualmente, lo representamos como un vector columna\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ es un parámetro escalar.  "
   ],
   "id": "a96c6ca2dfc935cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Para la demostración, $\\mathbf{w}$ y $b$ se cargarán con algunos valores iniciales seleccionados que están cerca del óptimo. $\\mathbf{w}$ es un vector NumPy de 1-D.",
   "id": "a3cfbf437917b82b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"forma de w_init: {w_init.shape}, tipo de b_init: {type(b_init)}\")"
   ],
   "id": "50bb5c2053a4547d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# 3 Predicción del Modelo con Múltiples Variables\n",
    "La predicción del modelo con múltiples variables se da por el modelo lineal:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "o en notación vectorial:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "donde $\\cdot$ es un `producto punto` vectorial\n",
    "\n",
    "Para demostrar el producto punto, implementaremos la predicción usando (1) y (2)."
   ],
   "id": "157384548d1f9c06"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_3.1\"></a>\n",
    "## 3.1 Predicción individual elemento por elemento\n",
    "Nuestra predicción anterior multiplicaba un valor de característica por un parámetro y añadía un parámetro de sesgo. Una extensión directa de nuestra implementación anterior de la predicción a múltiples características sería implementar (1) anterior usando un bucle sobre cada elemento, realizando la multiplicación con su parámetro y luego añadiendo el parámetro de sesgo al final."
   ],
   "id": "26fd0e1c6f325e38"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    predicción única usando regresión lineal\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Forma (n,) ejemplo con múltiples características\n",
    "      w (ndarray): Forma (n,) parámetros del modelo    \n",
    "      b (escalar):  parámetro del modelo     \n",
    "      \n",
    "    Returns:\n",
    "      p (escalar):  predicción\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p\n"
   ],
   "id": "d0acfc85383a1a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# obtener una fila de nuestros datos de entrenamiento\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"forma de x_vec {x_vec.shape}, valor de x_vec: {x_vec}\")\n",
    "\n",
    "# hacer una predicción\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"forma de f_wb {f_wb.shape}, predicción: {f_wb}\")\n"
   ],
   "id": "4b4aaed0b6e3b6ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Nota la forma de `x_vec`. Es un vector NumPy 1-D con 4 elementos, (4,). El resultado, `f_wb`, es un escalar.",
   "id": "210a2bfbcfbc9c3d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "## 3.2 Predicción individual, vector\n",
    "\n",
    "Observando que la ecuación (1) anterior puede implementarse usando el producto punto como en (2) arriba. Podemos hacer uso de operaciones vectoriales para acelerar las predicciones.\n",
    "\n",
    "Recuerda del laboratorio de Python/NumPy que NumPy `np.dot()`[[enlace](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] puede usarse para realizar un producto punto vectorial."
   ],
   "id": "13a3ac75953f54ce"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    predicción única usando regresión lineal\n",
    "    Args:\n",
    "      x (ndarray): Forma (n,) ejemplo con múltiples características\n",
    "      w (ndarray): Forma (n,) parámetros del modelo   \n",
    "      b (escalar): parámetro del modelo \n",
    "      \n",
    "    Returns:\n",
    "      p (escalar):  predicción\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    \n"
   ],
   "id": "c617c4f66966e2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# obtener una fila de nuestros datos de entrenamiento\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"forma de x_vec {x_vec.shape}, valor de x_vec: {x_vec}\")\n",
    "\n",
    "# hacer una predicción\n",
    "f_wb = predict(x_vec, w_init, b_init)\n",
    "print(f\"forma de f_wb {f_wb.shape}, predicción: {f_wb}\")\n"
   ],
   "id": "7378045bb0b6111b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Los resultados y las formas son los mismos que en la versión anterior que utilizaba bucles. En adelante, se utilizará `np.dot` para estas operaciones. La predicción es ahora una sola declaración. La mayoría de las rutinas la implementarán directamente en lugar de llamar a una rutina de predicción separada.",
   "id": "6ee161798eda4c18"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Calcular Costo con Múltiples Variables\n",
    "La ecuación para la función de costo con múltiples variables $J(\\mathbf{w},b)$ es:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "donde:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "A diferencia de laboratorios anteriores, $\\mathbf{w}$ y $\\mathbf{x}^{(i)}$ son vectores en lugar de escalares, lo que permite soportar múltiples características."
   ],
   "id": "b2e6cbcb16aa6f90"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "A continuación se muestra una implementación de las ecuaciones (3) y (4). Ten en cuenta que esto utiliza un *patrón estándar para este curso* donde se usa un bucle for sobre todos los ejemplos `m`.",
   "id": "494860ee31e578fa"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    calcular costo\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Datos, m ejemplos con n características\n",
    "      y (ndarray (m,)) : valores objetivo\n",
    "      w (ndarray (n,)) : parámetros del modelo  \n",
    "      b (escalar)      : parámetro del modelo\n",
    "      \n",
    "    Returns:\n",
    "      cost (escalar): costo\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = escalar (ver np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #escalar\n",
    "    cost = cost / (2 * m)                      #escalar    \n",
    "    return cost\n"
   ],
   "id": "5187d7dca458cb24",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Calcular y mostrar el costo usando nuestros parámetros óptimos preseleccionados.\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Costo con w óptimo: {cost}')"
   ],
   "id": "5ff0cc7476ae2c0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "**Resultado Esperado**: Costo con w óptimo: 1.5578904045996674e-12",
   "id": "a96e54f5210758e9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 Descenso del Gradiente con Múltiples Variables\n",
    "Descenso del gradiente para múltiples variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repetir}&\\text{ hasta converger:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{para j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "donde, n es el número de características, los parámetros $w_j$,  $b$, se actualizan simultáneamente y donde  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m es el número de ejemplos de entrenamiento en el conjunto de datos\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ es la predicción del modelo, mientras que $y^{(i)}$ es el valor objetivo"
   ],
   "id": "8631b94355ddd64"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Calcular el Gradiente con Múltiples Variables\n",
    "A continuación se presenta una implementación para calcular las ecuaciones (6) y (7). Hay muchas maneras de implementar esto. En esta versión, hay un\n",
    "- bucle externo sobre todos los ejemplos m.\n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ para el ejemplo se puede calcular directamente y acumular\n",
    "    - en un segundo bucle sobre todas las características n:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ se calcula para cada $w_j$."
   ],
   "id": "24a385f14da6c4a9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula el gradiente para la regresión lineal \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Datos, m ejemplos con n características\n",
    "      y (ndarray (m,)) : valores objetivo\n",
    "      w (ndarray (n,)) : parámetros del modelo  \n",
    "      b (escalar)      : parámetro del modelo\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): El gradiente del costo respecto a los parámetros w. \n",
    "      dj_db (escalar):       El gradiente del costo respecto al parámetro b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape           #(número de ejemplos, número de características)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw\n"
   ],
   "id": "d317259af8a6985a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Calcular y mostrar el gradiente\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db con w,b iniciales: {tmp_dj_db}')\n",
    "print(f'dj_dw con w,b iniciales: \\n {tmp_dj_dw}')"
   ],
   "id": "95171abdf87a1eef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Resultado Esperado**:   \n",
    "dj_db con w,b iniciales: -1.6739251122999121e-06  \n",
    "dj_dw con w,b iniciales:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ],
   "id": "277deb026f753b87"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_5.2\"></a>\n",
    "## 5.2 Descenso del Gradiente con Múltiples Variables\n",
    "La rutina a continuación implementa la ecuación (5) mencionada anteriormente."
   ],
   "id": "3d7c35a7d2d594b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Realiza descenso de gradiente por lotes para aprender theta. Actualiza theta tomando \n",
    "    num_iters pasos de gradiente con la tasa de aprendizaje alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Datos, m ejemplos con n características\n",
    "      y (ndarray (m,))    : valores objetivo\n",
    "      w_in (ndarray (n,)) : parámetros iniciales del modelo  \n",
    "      b_in (escalar)      : parámetro inicial del modelo\n",
    "      cost_function       : función para calcular el costo\n",
    "      gradient_function   : función para calcular el gradiente\n",
    "      alpha (float)       : Tasa de aprendizaje\n",
    "      num_iters (int)     : número de iteraciones para ejecutar el descenso de gradiente\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Valores actualizados de los parámetros \n",
    "      b (escalar)      : Valor actualizado del parámetro \n",
    "      \"\"\"\n",
    "    \n",
    "    # Un arreglo para almacenar el costo J y los valores de w en cada iteración principalmente para graficar luego\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #evitar modificar el w global dentro de la función\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calcular el gradiente y actualizar los parámetros\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Actualizar parámetros usando w, b, alpha y gradiente\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Guardar el costo J en cada iteración\n",
    "        if i<100000:      # evitar agotamiento de recursos \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Imprimir el costo cada cierto intervalo, 10 veces o tantas iteraciones si son < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteración {i:4d}: Costo {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #devolver el w, b y el historial de J finales para graficar\n"
   ],
   "id": "bc271301d30d9af8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "En la siguiente celda probarás la implementación.",
   "id": "62533dbe6532505b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# inicializar parámetros\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# configuraciones de descenso de gradiente\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# ejecutar descenso de gradiente \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b, w encontrados por descenso de gradiente: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"predicción: {np.dot(X_train[i], w_final) + b_final:0.2f}, valor objetivo: {y_train[i]}\")\n"
   ],
   "id": "7998fcfa99f6907b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Resultado Esperado**:    \n",
    "b, w encontrados por descenso de gradiente: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "predicción: 426.19, valor objetivo: 460  \n",
    "predicción: 286.17, valor objetivo: 232  \n",
    "predicción: 171.47, valor objetivo: 178  "
   ],
   "id": "994ad1013eb05686"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# graficar costo versus iteración\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Costo vs. iteración\");  ax2.set_title(\"Costo vs. iteración (final)\")\n",
    "ax1.set_ylabel('Costo')             ;  ax2.set_ylabel('Costo') \n",
    "ax1.set_xlabel('paso de iteración')   ;  ax2.set_xlabel('paso de iteración') \n",
    "plt.show()\n"
   ],
   "id": "ae054eab7c53c8aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "*¡Estos resultados no son inspiradores*! El costo aún está disminuyendo y nuestras predicciones no son muy precisas. El próximo laboratorio explorará cómo mejorar esto.",
   "id": "dcdefc6cdf0be67f"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"toc_15456_6\"></a>\n",
    "# ¡6 Felicitaciones!\n",
    "En este laboratorio:\n",
    "- Desarrollaste de nuevo las rutinas para la regresión lineal, ahora con múltiples variables.\n",
    "- Utilizaste NumPy `np.dot` para vectorizar las implementaciones."
   ],
   "id": "d89c9293c3e2116e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "id": "3d3cd58bf4726d8b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
