{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wakusoftware/curso-ml-espanol/blob/master/C1%20-%20Aprendizaje%20Supervisado/W2/C1_W2_Lab04_IngCars_RegPol.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "id": "9d4c1907542beca4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup para Colab\n",
    "Si estás corriendo este Notebook en Google Colab corre la celda de abajo, de lo contrario ignórala."
   ],
   "id": "7f9d573cefd08d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/wakusoftware/curso-ml-espanol.git\n",
    "\n",
    "%cd curso-ml-espanol/C1 - Aprendizaje Supervisado/W2/\n",
    "\n",
    "!cp lab_utils_common.py /content/\n",
    "\n",
    "!cp lab_utils_multi.py /content/\n",
    "\n",
    "!cp deeplearning.mplstyle /content/\n",
    "\n",
    "!cp -r data /content/\n",
    "\n",
    "!cp -r images /content/\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "!rm -rf curso-ml-espanol/"
   ],
   "id": "5462fea51b16e165",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "# Laboratorio: Ingeniería de características y regresión polinómica",
   "id": "f0717531306ed2d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Objetivos\n",
    "En este laboratorio realizarás:\n",
    "- exploración de la ingeniería de características y la regresión polinómica, lo que te permite usar la maquinaria de la regresión lineal para ajustar funciones muy complicadas, incluso muy no lineales."
   ],
   "id": "518ecda37f1c6a44"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Herramientas\n",
    "Utilizarás la función desarrollada en laboratorios anteriores, así como matplotlib y NumPy."
   ],
   "id": "bd3b80862b5f41da"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import zscore_normalize_features, run_gradient_descent_feng\n",
    "np.set_printoptions(precision=2)  # precisión reducida en la visualización de arreglos de numpy"
   ],
   "id": "21b4877afb180bde",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='FeatureEng'></a>\n",
    "# Visión General de la Ingeniería de Características y la Regresión Polinómica\n",
    "\n",
    "De manera predeterminada, la regresión lineal ofrece un medio para construir modelos de la forma:\n",
    "$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "¿Qué sucede si tus características/datos no son lineales o son combinaciones de características? Por ejemplo, los precios de las viviendas no tienden a ser lineales con respecto al área habitable, sino que penalizan las casas muy pequeñas o muy grandes, resultando en las curvas mostradas en el gráfico anterior. ¿Cómo podemos usar la maquinaria de la regresión lineal para ajustar esta curva? Recuerda, la 'maquinaria' que tenemos es la capacidad de modificar los parámetros $\\mathbf{w}$, $\\mathbf{b}$ en (1) para 'ajustar' la ecuación a los datos de entrenamiento. Sin embargo, ningún ajuste de $\\mathbf{w}$, $\\mathbf{b}$ en (1) logrará un ajuste a una curva no lineal."
   ],
   "id": "e7941f0f6a40c93b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='PolynomialFeatures'></a>\n",
    "## Características Polinómicas\n",
    "\n",
    "Anteriormente consideramos un escenario donde los datos eran no lineales. Vamos a intentar usar lo que sabemos hasta ahora para ajustar una curva no lineal. Empezaremos con una cuadrática simple: $y = 1+x^2$\n",
    "\n",
    "Estás familiarizado con todas las rutinas que estamos utilizando. Están disponibles en el archivo lab_utils.py para revisión. Usaremos [`np.c_[..]`](https://numpy.org/doc/stable/reference/generated/numpy.c_.html) que es una rutina de NumPy para concatenar a lo largo del límite de la columna."
   ],
   "id": "64b26a56220ab8f0"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# crear datos objetivo\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=1000, alpha = 1e-2)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Valor real\"); plt.title(\"sin ingeniería de características\")\n",
    "plt.plot(x, X@model_w + model_b, label=\"Valor predicho\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ],
   "id": "552ebabe1c3b3d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bueno, como se esperaba, no es un buen ajuste. Lo que se necesita es algo como $y= w_0x_0^2 + b$, o una **característica polinómica**.\n",
    "Para lograr esto, puedes modificar los *datos de entrada* para *ingeniar* las características necesarias. Si cambias los datos originales por una versión que eleve al cuadrado el valor de $x$, entonces puedes lograr $y= w_0x_0^2 + b$. Vamos a intentarlo. Cambia `X` por `X**2` a continuación:"
   ],
   "id": "8ccc9a96114f19c1"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# crear datos objetivo\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "\n",
    "# Ingeniar características\n",
    "X = x**2      # <-- característica modificada agregada"
   ],
   "id": "4fa953fbb70c0250",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "X = X.reshape(-1, 1)  # X debe ser una matriz 2-D\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Valor actual\"); plt.title(\"Característica agregada x**2\")\n",
    "plt.plot(x, np.dot(X,model_w) + model_b, label=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ],
   "id": "b37a9625dcce698d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "¡Genial! Casi un ajuste perfecto. Observa los valores de $\\mathbf{w}$ y b impresos justo arriba del gráfico: `w, b encontrados por descenso de gradiente: w: [1.], b: 0.0490`. El descenso de gradiente modificó nuestros valores iniciales de $\\mathbf{w}, b$ para ser (1.0, 0.049) o un modelo de $y=1*x_0^2+0.049$, muy cercano a nuestro objetivo de $y=1*x_0^2+1$. Si lo ejecutaras más tiempo, podría ser una mejor coincidencia.",
   "id": "9896c830b5e1408"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Seleccionando Características\n",
    "<a name='GDF'></a>\n",
    "Anteriormente, sabíamos que se requería un término $x^2$. No siempre puede ser obvio qué características son necesarias. Se podrían añadir una variedad de características potenciales para intentar encontrar las más útiles. Por ejemplo, ¿qué pasaría si en lugar de eso hubiéramos intentado: $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$?\n",
    "\n",
    "Ejecuta las siguientes celdas."
   ],
   "id": "3501370b5e789291"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# crear datos objetivo\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# ingeniar características\n",
    "X = np.c_[x, x**2, x**3]   # <-- característica modificada agregada"
   ],
   "id": "e8f1a00579efc4c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model_w, model_b = run_gradient_descent_feng(X, y, iteraciones=10000, alpha=1e-7)\n",
    "\n",
    "plt.scatter(x, y, marcador='x', c='r', etiqueta=\"Valor actual\"); plt.title(\"Características x, x**2, x**3\")\n",
    "plt.plot(x, X@model_w + model_b, etiqueta=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ],
   "id": "1137d5463f2109b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observa el valor de $\\mathbf{w}$, `[0.08 0.54 0.03]` y b es `0.0106`. Esto implica que el modelo después de ajustar/entrenar es:\n",
    "$$ 0.08x + 0.54x^2 + 0.03x^3 + 0.0106 $$\n",
    "El descenso de gradiente ha enfatizado los datos que mejor se ajustan a los datos de $x^2$ aumentando el término $w_1$ en relación con los demás. Si se ejecutara durante mucho tiempo, continuaría reduciendo el impacto de los otros términos.\n",
    "> El descenso de gradiente está seleccionando las 'características correctas' para nosotros al enfatizar su parámetro asociado.\n",
    "\n",
    "Revisemos esta idea:\n",
    "- Inicialmente, las características fueron reescaladas para que sean comparables entre sí.\n",
    "- Un valor de peso menor implica una característica menos importante/correcta, y en extremo, cuando el peso se vuelve cero o muy cercano a cero, la característica asociada es útil para ajustar el modelo a los datos.\n",
    "- Arriba, después del ajuste, el peso asociado con la característica de $x^2$ es mucho mayor que los pesos para $x$ o $x^3$, ya que es la más útil para ajustar los datos."
   ],
   "id": "8216db578f025d81"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Una Visión Alternativa\n",
    "Arriba, las características polinómicas se eligieron según cuán bien coincidían con los datos objetivo. Otra forma de pensar en esto es notar que todavía estamos usando regresión lineal una vez que hemos creado nuevas características. Dado esto, las mejores características serán lineales en relación con el objetivo. Esto se entiende mejor con un ejemplo."
   ],
   "id": "390d571cc170994b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# crear datos objetivo\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# ingeniar características\n",
    "X = np.c_[x, x**2, x**3]   # <-- característica ingenierizada agregada\n",
    "X_features = ['x', 'x^2', 'x^3']"
   ],
   "id": "de638c333e122d32",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X[:,i],y)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"y\")\n",
    "plt.show()"
   ],
   "id": "99c62fb985b37cce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Arriba, es claro que la característica $x^2$ mapeada contra el valor objetivo $y$ es lineal. Entonces, la regresión lineal puede fácilmente generar un modelo usando esa característica.\n",
   "id": "1a11098e2d289446"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Escalado de características\n",
    "Como se describió en el último laboratorio, si el conjunto de datos tiene características con escalas significativamente diferentes, se debe aplicar el escalado de características para acelerar el descenso de gradiente. En el ejemplo anterior, hay $x$, $x^2$ y $x^3$ que naturalmente tendrán escalas muy diferentes. Apliquemos la normalización Z-score a nuestro ejemplo."
   ],
   "id": "742a619a36de1568"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# crear datos objetivo\n",
    "x = np.arange(0,20,1)\n",
    "X = np.c_[x, x**2, x**3]\n",
    "print(f\"Rango de pico a pico por columna en X crudo: {np.ptp(X, axis=0)}\")\n",
    "\n",
    "# añadir normalización media\n",
    "X = zscore_normalize_features(X)\n",
    "print(f\"Rango de pico a pico por columna en X normalizado: {np.ptp(X, axis=0)}\")"
   ],
   "id": "36212c9aef8ddcdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": "Ahora podemos intentarlo nuevamente con un valor más agresivo de alpha:",
   "id": "49b26a4ce8ff9ea9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "x = np.arange(0,20,1)\n",
    "y = x**2\n",
    "\n",
    "X = np.c_[x, x**2, x**3]\n",
    "X = zscore_normalize_features(X)\n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iteraciones=100000, alpha=1e-1)\n",
    "\n",
    "plt.scatter(x, y, marcador='x', c='r', etiqueta=\"Valor actual\"); plt.title(\"Característica normalizada x, x**2, x**3\")\n",
    "plt.plot(x, X@model_w + model_b, etiqueta=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ],
   "id": "46543da65a6ee4fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El escalado de características permite que esto converja mucho más rápido.\n",
    "Nota nuevamente los valores de $\\mathbf{w}$. El término $w_1$, que es el término $x^2$, es el más enfatizado. El descenso de gradiente ha eliminado casi por completo el término $x^3$."
   ],
   "id": "ec752c9dd8efd670"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Funciones Complejas\n",
    "Con la ingeniería de características, incluso funciones bastante complejas pueden ser modeladas:"
   ],
   "id": "9a818d92a2ebce60"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "x = np.arange(0,20,1)\n",
    "y = np.cos(x/2)\n",
    "\n",
    "X = np.c_[x, x**2, x**3, x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\n",
    "X = zscore_normalize_features(X)\n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iteraciones=1000000, alpha = 1e-1)\n",
    "\n",
    "plt.scatter(x, y, marcador='x', c='r', etiqueta=\"Valor actual\"); plt.title(\"Característica normalizada x, x**2, x**3\")\n",
    "plt.plot(x, X@model_w + model_b, etiqueta=\"Valor predicho\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ],
   "id": "54d52ec7c26c7ce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ¡Felicidades!\n",
    "En este laboratorio:\n",
    "- aprendiste cómo la regresión lineal puede modelar funciones complejas, incluso altamente no lineales, mediante la ingeniería de características\n",
    "- reconociste que es importante aplicar el escalado de características al realizar la ingeniería de características"
   ],
   "id": "f561faff0d318a70"
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "id": "e789ebe95ec80263",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
